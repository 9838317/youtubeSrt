<?xml version="1.0" encoding="utf-8" ?><transcript><text start="0.29" dur="2.86">This video provides an overview of some NoSQL systems.</text><text start="4.32" dur="2.96">I want to say right up front that it&amp;#39;s being made in November, 2011.</text><text start="8" dur="1.03">This is a field</text><text start="9.31" dur="1.64">that&amp;#39;s changing very fast, so</text><text start="11.4" dur="1.01">this is an overview of what&amp;#39;s</text><text start="12.66" dur="1.87">going on right now.</text><text start="14.7" dur="0.78">As a reminder from the previous</text><text start="15.96" dur="1.64">video, NoSQL systems have</text><text start="17.86" dur="1.5">arisen because it was recognized that</text><text start="19.61" dur="1.53">not every problem involving large</text><text start="21.5" dur="1.6">scale management or analysis of</text><text start="23.24" dur="1.59">data was best solved</text><text start="25.22" dur="1.4">by using a relational database system.</text><text start="27.43" dur="1.74">Some problems still are, but</text><text start="29.36" dur="0.99">there are others that are more suitable</text><text start="30.71" dur="1.95">for a different type of system that we&amp;#39;re going to talk about.</text><text start="33.45" dur="2.21">NoSQL as a term</text><text start="35.79" dur="1.25">has evolved to mean not</text><text start="37.5" dur="1.78">only SQL, where SQL</text><text start="39.48" dur="0.9">doesn&amp;#39;t really mean the SQL language,</text><text start="40.95" dur="2.09">but it means a traditional database management system.</text><text start="44.39" dur="0.95">Again, as a reminder from the</text><text start="45.46" dur="1.21">previous video, the NoSQL</text><text start="47.25" dur="1.45">systems are different from</text><text start="49.01" dur="1.29">traditional systems in that they</text><text start="50.52" dur="2.6">tend to provide a flexible schema rather than a rigid structure.</text><text start="54.09" dur="2.52">They tend to be quicker or cheaper, or both, to set up.</text><text start="57.15" dur="1.47">They&amp;#39;re geared towards really massive</text><text start="59.18" dur="1.13">scalability, and they tend</text><text start="60.38" dur="1.92">to use relaxed consistency models</text><text start="62.74" dur="2.57">in order to give higher performance and higher availability.</text><text start="66.34" dur="1.1">The downside is being that there&amp;#39;s</text><text start="67.65" dur="1.98">no declarative query language, so</text><text start="69.73" dur="1.2">more programming is typically involved</text><text start="71.77" dur="1.7">in manipulating the data, and</text><text start="73.79" dur="1.59">because of the relaxed consistency models,</text><text start="76.03" dur="1.34">the plus is a better</text><text start="77.57" dur="1.63">performance, the downside is</text><text start="79.39" dur="2.54">fewer guarantees about the consistency of the data.</text><text start="82.98" dur="0.67">So there are a number of</text><text start="84.14" dur="2.06">incarnations of NoSQL systems, and</text><text start="86.29" dur="1.38">I&amp;#39;ve chosen, as of November</text><text start="87.97" dur="1.62">2011 to divide into</text><text start="89.98" dur="1.51">four categories, the MapReduce</text><text start="91.84" dur="1.46">framework, key value stores,</text><text start="93.75" dur="2">document stores, and graph database systems.</text><text start="96.99" dur="1">In terms of the first two,</text><text start="98.17" dur="0.94">one way you can think</text><text start="99.29" dur="0.9">about it sort of roughly</text><text start="100.98" dur="1.65">is that the MapReduce framework is</text><text start="102.91" dur="2.01">typically used for applications that</text><text start="105.4" dur="1.24">would have used relational OLAP</text><text start="107.09" dur="1.37">or online analytical processing.</text><text start="109.34" dur="1.79">They tend to be analysis applications that</text><text start="111.3" dur="2.49">touch large amounts of the data to do complex analyses.</text><text start="114.93" dur="1.61">Whereas key value stores tend</text><text start="116.81" dur="1.13">to be more in the</text><text start="118.6" dur="1.67">OLTP world as a</text><text start="120.47" dur="2.29">reminder that&amp;#39;s online transaction processing and</text><text start="122.87" dur="1.27">that tends to be a</text><text start="124.42" dur="1.66">lot of small operations touching</text><text start="126.56" dur="1.07">very small parts of the data.</text><text start="128.21" dur="1.71">The other two document stores</text><text start="130.28" dur="1.83">and graph database systems are self-explanatory.</text><text start="132.76" dur="1.23">They involve documents and graphs.</text><text start="134.96" dur="1.06">Now you might wonder why</text><text start="136.37" dur="1.64">I didn&amp;#39;t mention column stores, because</text><text start="138.28" dur="2.44">column stores are often discussed in terms of NoSQL.</text><text start="141.37" dur="1.6">So, column stores are in</text><text start="143.13" dur="1.16">one sense just a way</text><text start="144.54" dur="1.89">of organizing relational database systems</text><text start="147.3" dur="1.26">for higher performance for certain types</text><text start="148.86" dur="1.45">of applications but we&amp;#39;ll also</text><text start="150.8" dur="1.66">see that key values stores do</text><text start="152.81" dur="0.95">tend to have, sometimes,</text><text start="154.34" dur="1.32">not all of them, have a</text><text start="155.92" dur="2.89">model that&amp;#39;s also based on columns being an important concept.</text><text start="159.99" dur="1.31">So, now I&amp;#39;ll discuss each of</text><text start="161.41" dur="1.24">these in turn, although I&amp;#39;m going</text><text start="162.77" dur="1.85">to spend the most amount of time on MapReduce.</text><text start="165.57" dur="1.9">So we can think of MapReduce as a framework.</text><text start="168.09" dur="1.09">It came originally from Google.</text><text start="169.7" dur="1.29">They invented the term MapReduce, and</text><text start="171.83" dur="1.22">now there&amp;#39;s an open source system</text><text start="173.43" dur="2.03">widely used called Hadoop which</text><text start="175.75" dur="1.68">does implement the MapReduce framework</text><text start="178.71" dur="1.06">so the first aspect of MapReduce</text><text start="180.37" dur="1.63">is that there is no data model at all.</text><text start="182.58" dur="1.31">The data in the MapReduce</text><text start="184.31" dur="1.41">framework is stored in files</text><text start="186.16" dur="1.14">both as input and output.</text><text start="188.08" dur="1.52">In the Google MapReduce</text><text start="189.97" dur="2.59">implementation, it&amp;#39;s the Google File System, GFS.</text><text start="193.59" dur="1.69">In the Hadoop open source</text><text start="195.28" dur="3.36">implementation, it&amp;#39;s the Hadoop Distributed File System, HDFS.</text><text start="199.83" dur="1.45">What the user provides to process</text><text start="201.79" dur="1.17">data using the MapReduce framework,</text><text start="203.76" dur="1.89">is a set of specific functions.</text><text start="206.57" dur="1.26">Not surprisingly, one of those</text><text start="208.07" dur="1.35">functions is called map, and</text><text start="209.76" dur="1.23">one of them is called reduce.</text><text start="212.31" dur="1.19">Other functions that the user</text><text start="213.73" dur="1.43">needs to provide is a</text><text start="215.37" dur="1.26">reader function which will</text><text start="216.84" dur="2.11">read data from files and provide it as records.</text><text start="219.9" dur="1.19">A writer function that will</text><text start="221.24" dur="1.65">take the output records and</text><text start="223.08" dur="1.51">write them into files, and finally</text><text start="225.01" dur="1.48">there&amp;#39;s an optional function called</text><text start="226.83" dur="1.2">the combiner, that we&amp;#39;ll discuss.</text><text start="229.19" dur="1.62">So, the user just provides this</text><text start="231" dur="1.32">set of functions, and then what</text><text start="232.46" dur="1.58">the system provides, is the</text><text start="234.28" dur="2.8">glue that processes the data through the functions.</text><text start="237.65" dur="1.58">The system also provides fault tolerance</text><text start="239.8" dur="2.06">of the processing, so, if there is</text><text start="242.17" dur="0.89">a crash or a node</text><text start="243.33" dur="1.3">goes down during the execution,</text><text start="245.33" dur="1.85">it will be guaranteed to be as if that didn&amp;#39;t happen.</text><text start="248" dur="1.2">And finally the system also provides</text><text start="249.66" dur="2.01">scalability so that the</text><text start="251.76" dur="1">MapReduce framework can be</text><text start="252.86" dur="1.67">used for very very large data analysis.</text><text start="255.74" dur="0.81">So let&amp;#39;s talk about the two</text><text start="256.72" dur="2.68">most important functions, the map function and the reduce function.</text><text start="260.43" dur="1.15">The map function is used</text><text start="261.97" dur="2.07">to take the data analysis problem</text><text start="264.68" dur="0.99">and divide it into sub-problems.</text><text start="266.71" dur="1.8">Very specifically the function that</text><text start="268.71" dur="1.93">the user provides called map is</text><text start="270.75" dur="1.11">going to take a data item</text><text start="272.39" dur="1.54">as input and it&amp;#39;s</text><text start="274.13" dur="3.58">going to produce as output zero or more key value pairs.</text><text start="279.03" dur="0.99">Now what I mean by a</text><text start="280.55" dur="1.19">sub-problem here is that we&amp;#39;re going</text><text start="282.36" dur="1.6">to separately deal with the</text><text start="284.33" dur="1.67">set of records associated with</text><text start="286.18" dur="2.53">each key, and that&amp;#39;s the job of the reduce function.</text><text start="289.59" dur="1.88">So the reduce function, which we&amp;#39;ll</text><text start="291.67" dur="1.85">write, takes as its</text><text start="293.78" dur="2.04">parameters a key and</text><text start="296.02" dur="1.38">then a list of values for</text><text start="297.52" dur="2.11">that key and it</text><text start="299.87" dur="1.98">produces as output, zero or more records.</text><text start="303.3" dur="1.05">Now we&amp;#39;ll shortly see a concrete</text><text start="304.8" dur="1.65">example that will, hopefully, make</text><text start="306.63" dur="1.56">this more understandable but before</text><text start="308.5" dur="1.31">we do that, let &amp;#39;s look at the</text><text start="310.01" dur="1.4">overall architecture of how</text><text start="311.69" dur="2.27">these functions are used to process data.</text><text start="315.19" dur="1.1">So we&amp;#39;ll start with our map</text><text start="316.49" dur="1.14">function, which, let&amp;#39;s put inside</text><text start="318.07" dur="1.35">a box, and then we</text><text start="319.58" dur="2.78">will have input records going into the map function.</text><text start="323.68" dur="1.21">As a reminder, what the map</text><text start="325.26" dur="1.48">function produces from each input</text><text start="327.16" dur="1.44">record is an output record</text><text start="328.7" dur="2.04">that&amp;#39;s a key value pair and</text><text start="330.85" dur="1.15">we&amp;#39;re going to have these</text><text start="332.34" dur="1.99">records sort of directed</text><text start="335.14" dur="1.3">in a different way for each key.</text><text start="336.68" dur="1.27">So let&amp;#39;s say this is</text><text start="338.17" dur="1.08">the way that the records are</text><text start="339.38" dur="1.19">gonna go for key 1, key</text><text start="341.06" dur="2.03">2, and up to key n.</text><text start="343.39" dur="1.06">And of course the records will</text><text start="344.6" dur="1.65">have values associated with them as well.</text><text start="347.27" dur="1.73">So we&amp;#39;ll send each batch of</text><text start="349.16" dur="1.59">records for a given</text><text start="351.28" dur="1.32">key into our reduce</text><text start="353.07" dur="0.76">function, so let me just</text><text start="354.03" dur="1.2">draw a few reduce boxes</text><text start="355.68" dur="1.4">here, there&amp;#39;s one for each</text><text start="357.62" dur="1.41">set of records for a given key.</text><text start="360.66" dur="1.51">And then as we mentioned before the</text><text start="362.3" dur="2.1">reduce function produces output records.</text><text start="365.92" dur="2.3">At the highest level, that&amp;#39;s it. That&amp;#39;s our data processing.</text><text start="368.85" dur="0.93">We start with a bunch of input.</text><text start="370.45" dur="1.4">We divide it up</text><text start="372.23" dur="1.68">into sub-problems, based on a</text><text start="374.44" dur="1.29">key, which will extract from the</text><text start="375.91" dur="1.2">input record somehow, we&amp;#39;ll see an</text><text start="377.17" dur="1.42">example, and then each</text><text start="379.12" dur="1.93">sub-problem, associated with a</text><text start="381.23" dur="1.43">particular key is set through</text><text start="382.8" dur="1.43">the reduce function, which produces the output.</text><text start="384.84" dur="0.89">And that&amp;#39;s the end of our processing.</text><text start="387.05" dur="2.72">Now things are, of course, a bit more complex than that.</text><text start="389.9" dur="1.06">First of all, there&amp;#39;s no reason</text><text start="391.35" dur="1.52">to have one map box, because</text><text start="393.18" dur="1.41">the map function takes each input</text><text start="395" dur="1.3">record and processes it</text><text start="396.44" dur="1.41">separately, so we can parallelize</text><text start="398.35" dur="1.95">the mapping as much as we want.</text><text start="400.58" dur="1.36">So let&amp;#39;s change the picture here,</text><text start="402.16" dur="2.02">to have a whole set of map boxes.</text><text start="405.43" dur="1.37">So now, each MapBox is</text><text start="406.92" dur="1.28">going to take its records and</text><text start="408.38" dur="1.55">it&amp;#39;s going to produce records with</text><text start="410.21" dur="2.99">given keys so we&amp;#39;ll still send k1 over to the first reducer.</text><text start="413.5" dur="1.42">If we have k2</text><text start="415.34" dur="1.45">it&amp;#39;ll go here and down here.</text><text start="417.55" dur="0.17">And of course.</text><text start="418.03" dur="1.17">this map will send things to</text><text start="419.32" dur="2">reduce, reduce, reduce, and so on.</text><text start="422.31" dur="1.03">Now, you might wonder what happened</text><text start="423.9" dur="1.38">to those reader and writer</text><text start="425.67" dur="1.17">functions that I talked about.</text><text start="427.5" dur="0.97">The reality is that we don&amp;#39;t</text><text start="428.77" dur="1.38">actually start with input records, we</text><text start="430.24" dur="1.12">start with our data in</text><text start="431.71" dur="2.6">files. So here&amp;#39;s the real original data.</text><text start="434.58" dur="1.19">We&amp;#39;ll draw this picture here</text><text start="436.02" dur="2.13">for files, and let&amp;#39;s</text><text start="438.39" dur="1.4">erase our input records here</text><text start="440.8" dur="1.63">because the job of</text><text start="442.87" dur="1.72">the reader is to take</text><text start="445.08" dur="1.96">the files, extract the</text><text start="447.13" dur="2.06">records from the files, and provide them to the map functions.</text><text start="450.6" dur="2.7">So here is that side of thing, it&amp;#39;s a bit sloppy but I think get the idea.</text><text start="454.27" dur="0.95">And we have a similar thing</text><text start="455.34" dur="1.02">on the other end, the output</text><text start="456.61" dur="0.75">methods come out of the</text><text start="457.46" dur="1.31">reducers, but then their</text><text start="458.95" dur="1.31">provided to the writer functions</text><text start="460.75" dur="2.33">that which write the output to a final file.</text><text start="464.12" dur="1.43">So here it is, our original</text><text start="465.99" dur="1.58">input in files here, our</text><text start="467.87" dur="1.79">final output in files there.</text><text start="470.57" dur="1.22">Ok, but let me remind</text><text start="471.83" dur="2.21">you what the user provide what the system provides.</text><text start="474.83" dur="2.07">So the user creates a single</text><text start="477.56" dur="1.55">map function that takes records</text><text start="479.66" dur="2.82">and emits a key value pair for each record.</text><text start="483.44" dur="1.39">The user provides a single reduce</text><text start="485.28" dur="0.8">function that takes a set</text><text start="486.1" dur="2.54">of values for a given</text><text start="489.3" dur="1.61">key and produces zero or</text><text start="490.96" dur="0.8">more outputs and I should</text><text start="491.9" dur="0.8">mention that the map can produce</text><text start="493.05" dur="1.72">zero or more outputs from each record as well.</text><text start="494.98" dur="1.23">It doesn&amp;#39;t have to be a one-to-one mapping.</text><text start="496.76" dur="1.51">The user also provides the</text><text start="498.48" dur="1.35">reader function, to extract data</text><text start="500.06" dur="0.92">from files and the writer</text><text start="501.32" dur="2.14">function, to write data to the output.</text><text start="503.85" dur="2.86">And there&amp;#39;s one more optional function I mentioned called the combiner.</text><text start="507.65" dur="1.72">The combiner, actually, is sort</text><text start="509.69" dur="0.83">of attached to the mapper,</text><text start="511.28" dur="1.32">so we can kind of put it here.</text><text start="514.03" dur="1.34">And what the combiner does is,</text><text start="515.48" dur="1.49">it actually, in sort of,</text><text start="517.24" dur="1.23">in the mapper, will take</text><text start="518.8" dur="1.35">a set of records for</text><text start="520.45" dur="1.27">a given key, so, say,</text><text start="521.97" dur="1.89">for K1 and then we&amp;#39;ll</text><text start="524.57" dur="3.24">send a combined version of that record to the reducer.</text><text start="528.66" dur="0.82">In a way, you can think of</text><text start="529.63" dur="1.59">it as a sort of pre-reduce phase,</text><text start="531.58" dur="1.07">and we&amp;#39;ll see examples of this</text><text start="533.13" dur="1.09">that occurs with the mapper,</text><text start="534.62" dur="2.32">to make things more efficient and send less data to the reducer.</text><text start="538.03" dur="1.41">So, the user has provided these</text><text start="539.68" dur="2.06">pieces, these system infrastructure</text><text start="541.89" dur="1.84">takes the pieces, and distributes</text><text start="544.54" dur="1.69">them to multiple machines, because</text><text start="546.44" dur="1.38">a lot of this can go on in parallel.</text><text start="548.33" dur="2.51">All of this can go on in parallel, this too, and this too.</text><text start="551.13" dur="1.34">Here you have to exchange</text><text start="552.81" dur="1.02">data, maybe from one machine</text><text start="554.25" dur="1.16">to another, but once you</text><text start="555.51" dur="2.53">do, parallelism can occur and here as well.</text><text start="558.64" dur="1.4">So the system distributes them to</text><text start="560.12" dur="0.84">machines, and you can add</text><text start="561.16" dur="1.57">more machines to make it all all run faster.</text><text start="563.5" dur="1.51">The system also provides fault tolerance,</text><text start="565.5" dur="1.54">so if something goes badly here,</text><text start="567.23" dur="1.64">it will redo that reducer function</text><text start="569.56" dur="2.26">and here as well, and finally,</text><text start="572.37" dur="1.5">as I mentioned before, it provides scalability.</text><text start="574.8" dur="0.96">But I should add, I think</text><text start="575.87" dur="1.12">one of the most important things the</text><text start="577.06" dur="3.51">mass produce architecture provides, is the glue that puts this all together.</text><text start="581.29" dur="0.92">Because again, the user is</text><text start="582.3" dur="1.44">only providing these functions, and</text><text start="583.86" dur="1.4">the system will take care of</text><text start="585.33" dur="1.6">all of the execution, moving the</text><text start="587.01" dur="1.26">data around and calling the</text><text start="588.4" dur="2.73">function over the large amounts of data that are being processed.</text><text start="592.67" dur="0.96">Well, all of that is pretty</text><text start="593.9" dur="1.07">abstract, so let&amp;#39;s look at</text><text start="595.05" dur="1.32">a concrete example, and let&amp;#39;s</text><text start="596.57" dur="1.39">go back to the domain that</text><text start="598.09" dur="1.3">I introduced in the previous video</text><text start="599.7" dur="1.26">of analyzing a web log,</text><text start="601.54" dur="1.02">where we have, in each record,</text><text start="603.1" dur="1.68">a user ID, URL, the</text><text start="604.87" dur="2.55">time of the access, and maybe some additional information.</text><text start="608.45" dur="0.99">And let&amp;#39;s start out with a</text><text start="609.7" dur="1.6">fairly simple task, which is</text><text start="611.42" dur="1.2">that we want to count the</text><text start="612.72" dur="1.87">number of accesses for each</text><text start="614.9" dur="2.21">domain, where the domain is inside the URL.</text><text start="617.75" dur="1">So, for example, the domain</text><text start="619.19" dur="2.11">might be the stanford.edu domain, where</text><text start="621.57" dur="1.32">we have accesses to many</text><text start="623.32" dur="1.34">different URLs with that domain</text><text start="625.28" dur="0.75">and we&amp;#39;re just going to count how</text><text start="626.22" dur="1.55">many accesses there have been to Stanford.</text><text start="629.35" dur="1.34">So to perform this task, the</text><text start="630.77" dur="1.09">user has to provide a</text><text start="632.11" dur="1.51">map function and a reduce function.</text><text start="634.17" dur="0.85">Let&amp;#39;s look at what they do.</text><text start="635.66" dur="1.99">The map function is going to take a record.</text><text start="638.04" dur="0.99">We&amp;#39;ll assume that the reader</text><text start="639.38" dur="1.44">has already extracted the record</text><text start="640.98" dur="1.33">from the file and it provides it</text><text start="642.82" dur="1.63">in this format with these four fields.</text><text start="645.37" dur="1.2">And what the map function is</text><text start="646.66" dur="1.17">going to do is simply look</text><text start="648" dur="1.4">inside the record and extract</text><text start="650.19" dur="1.25">the domain from the URL,</text><text start="652.44" dur="1.04">and it&amp;#39;s going to produce as</text><text start="653.81" dur="1.29">output from that record</text><text start="655.66" dur="1.56">the domain as the key,</text><text start="657.77" dur="1.27">so this is the key, and then</text><text start="659.68" dur="0.76">for this, we can just</text><text start="660.65" dur="1.14">have a null value as the</text><text start="661.85" dur="2.01">value, we&amp;#39;re not going to actually need to use a value.</text><text start="664.86" dur="1.75">And so that&amp;#39;s the job of the mapper, pretty simple.</text><text start="667.48" dur="1.54">Now what does the reduce function do?</text><text start="669.26" dur="1.29">The reduce function is going</text><text start="670.7" dur="2.05">to take a domain, because</text><text start="673" dur="0.95">that&amp;#39;s the key and that&amp;#39;s</text><text start="674.15" dur="1.12">the first argument, and then it&amp;#39;s</text><text start="675.41" dur="0.9">going to take a list of values,</text><text start="676.89" dur="0.89">in this case, it&amp;#39;s going to be</text><text start="677.85" dur="2.29">a list of null values, and</text><text start="680.28" dur="1.26">what&amp;#39;s interesting is that</text><text start="681.6" dur="0.86">each one of these null values</text><text start="682.81" dur="1.59">represents one access to that domain.</text><text start="684.98" dur="1.29">So all the reduce function</text><text start="686.5" dur="1.27">needs to do, is count up</text><text start="688.11" dur="1.41">how many nulls there are for</text><text start="689.6" dur="1.35">each domain, so it&amp;#39;s going</text><text start="691.16" dur="1.49">to produce as its result,</text><text start="693.46" dur="1.96">the domain and the count.</text><text start="697.56" dur="0.86">And believe it or not, we&amp;#39;ve</text><text start="698.6" dur="1.17">solved their problem with just</text><text start="700.02" dur="1.24">a little bit of code, just</text><text start="701.53" dur="1.35">a code to find the</text><text start="703" dur="1.26">domain inside the URL from</text><text start="704.43" dur="1.18">our record, and then this</text><text start="705.94" dur="2">simple code to count up the number of NULLs.</text><text start="708.55" dur="1.56">The system will take care of</text><text start="710.85" dur="1.62">shipping the records to</text><text start="712.61" dur="1.44">the right nodes to perform the</text><text start="714.13" dur="1.64">tasks in parallel and then</text><text start="716.12" dur="1.36">re-shipping them, so all of</text><text start="718.03" dur="1.21">the records, for all of</text><text start="719.3" dur="1.45">the outputs for a particular</text><text start="720.94" dur="1.54">domain, are in the same place and can be counted.</text><text start="722.94" dur="1.37">Now let me give an example</text><text start="724.99" dur="1.81">of how that combiner function will be used.</text><text start="727.38" dur="1.14">The combiner function as a reminder</text><text start="729.4" dur="0.98">will operate at the same</text><text start="730.62" dur="1.14">node as a mapper and</text><text start="731.93" dur="1.99">do some sort of pre-aggregation of the data.</text><text start="734.7" dur="1.69">So for example, we could</text><text start="736.55" dur="1.24">use a combiner, we&amp;#39;ll put</text><text start="738.01" dur="1.28">that right here after the</text><text start="739.35" dur="1.07">mapper and the combined</text><text start="741.07" dur="1.45">function is going to</text><text start="742.63" dur="2.51">take the domain and</text><text start="745.25" dur="1.2">the list of NULLs, actually it&amp;#39;s</text><text start="746.64" dur="1.25">going to do exactly what the</text><text start="748.49" dur="1.28">reduce function was doing</text><text start="750.95" dur="1.89">and it&amp;#39;s going to produce the domain and account.</text><text start="754.82" dur="1.5">And so that at each individual</text><text start="756.92" dur="1.04">node we&amp;#39;ll count up how many</text><text start="758.87" dur="1.04">accesses there were to that</text><text start="760.06" dur="1.36">domain in the data that&amp;#39;s</text><text start="761.61" dur="1.71">being processed at that node, but</text><text start="763.46" dur="0.74">then when we get to the</text><text start="764.27" dur="1.14">reduce function we may get</text><text start="765.74" dur="2.08">a bunch of</text><text start="768.07" dur="1.26">those records, so this list</text><text start="769.62" dur="1.34">of NULL here now becomes a</text><text start="771.27" dur="1">count that&amp;#39;s what arrives</text><text start="772.78" dur="1.02">at the reduce function, the output</text><text start="774.16" dur="1.46">of the combine, and then instead</text><text start="776.04" dur="1.23">of doing a count here we</text><text start="777.39" dur="1.4">do a sum and that</text><text start="778.89" dur="0.81">will give us the right answer</text><text start="780.06" dur="0.94">as well and that will be</text><text start="781.16" dur="1.21">more efficient again because of</text><text start="782.43" dur="1.97">the pre-aggregation that occurs right</text><text start="784.56" dur="2.03">in the same node that&amp;#39;s processing the map function.</text><text start="787.7" dur="2.31">Whoops, I made one mistake there. Sorry about that.</text><text start="790.49" dur="1.17">Actually this count here that</text><text start="791.76" dur="1.19">goes to the reduce function is</text><text start="793.32" dur="1.15">a list of counts,</text><text start="795.24" dur="0.75">right, because we&amp;#39;re going to get</text><text start="796.08" dur="1.48">one of these from each</text><text start="797.75" dur="2.72">of the mappers, and then we add those list of counts.</text><text start="800.79" dur="2.53">That&amp;#39;s the sum that we perform here, sorry about that small mistake.</text><text start="804.67" dur="1.14">Now let&amp;#39;s modify the problem.</text><text start="806.69" dur="1.6">We&amp;#39;ll take the same data but</text><text start="808.42" dur="1">instead of just counting how</text><text start="809.63" dur="1.23">many accesses we have to</text><text start="811.07" dur="1.77">each domain, let&amp;#39;s compute some</text><text start="813.1" dur="2.29">total value of the accesses for each domain.</text><text start="815.56" dur="1.35">And we might do</text><text start="817.04" dur="1.22">that based on something that we</text><text start="818.39" dur="1.48">see in the additional information, for</text><text start="820.08" dur="1.71">example, how valuable the</text><text start="822.01" dur="1.1">user is, whether the user went</text><text start="823.32" dur="1.59">off and bought something, something like that.</text><text start="825.72" dur="1.22">So let&amp;#39;s modify our map and</text><text start="827.03" dur="2.12">reduce functions for this slightly enhanced problem.</text><text start="830.5" dur="1.1">Now our map function again</text><text start="831.97" dur="1.27">is going to take a record, and</text><text start="833.45" dur="1">this time it&amp;#39;s not going to</text><text start="834.52" dur="0.86">look only at the URL,</text><text start="835.91" dur="0.72">but it&amp;#39;s also going to look</text><text start="836.97" dur="2.2">inside the additional information, and</text><text start="839.29" dur="1.28">what it will produce, is the</text><text start="840.65" dur="0.95">domain that it extracted</text><text start="842.32" dur="1.32">from the URL and then</text><text start="843.79" dur="1.08">let&amp;#39;s say some kind of score</text><text start="845.25" dur="1.63">on how valuable that access</text><text start="847.03" dur="2.15">was, based on whatever it sees inside additional information.</text><text start="850.73" dur="1.6">The reduced function, then, is</text><text start="852.49" dur="1.47">going to take a domain,</text><text start="854.94" dur="0.99">and it&amp;#39;s going to take a list</text><text start="856.48" dur="2.07">of scores for that</text><text start="858.78" dur="1.51">domain and then, similar</text><text start="860.84" dur="1.76">to what we had previously, the output</text><text start="862.87" dur="0.79">is going to be the domain</text><text start="864.51" dur="1.29">and the sum of those scores.</text><text start="867.03" dur="1.33">Now, one of the interesting things</text><text start="868.75" dur="1.34">here is, how the map function</text><text start="870.51" dur="1.25">interacts with this additional information,</text><text start="873.16" dur="0.49">because the map function is going</text><text start="873.7" dur="1.32">to have code, that is going</text><text start="875.14" dur="1.33">to look in the information and</text><text start="876.62" dur="2.16">it&amp;#39;s going to determine a score based on what it sees.</text><text start="879.95" dur="1.43">If we change what&amp;#39;s available</text><text start="881.84" dur="1.18">in additional information, then we</text><text start="883.3" dur="0.97">can modify the map function,</text><text start="884.57" dur="1.02">but everything else can stay</text><text start="885.8" dur="0.94">the same, or if we</text><text start="886.85" dur="2.09">say we refine how we extract the score.</text><text start="889.32" dur="1.76">So that is one benefit, to</text><text start="891.21" dur="1.23">some extent, of the the</text><text start="892.58" dur="1.33">MapReduce framework, because the</text><text start="894.34" dur="1.09">computation of the score is</text><text start="895.72" dur="1.61">just embedded in this one piece of code.</text><text start="898.34" dur="1.52">Now let&amp;#39;s modify our example further,</text><text start="900.56" dur="1.23">similar to the modification we</text><text start="902.02" dur="1.67">made in the earlier video, let&amp;#39;s</text><text start="903.87" dur="1.08">suppose that in addition to</text><text start="905.04" dur="2.62">the web blog we have separate information about the user.</text><text start="908.01" dur="1.15">So, separately from what might</text><text start="909.34" dur="1.39">be an additional info, we have</text><text start="910.96" dur="1.28">in a different data set,</text><text start="912.37" dur="2.66">the user ID, the name, the age, the gender and so forth.</text><text start="915.73" dur="0.99">And now let&amp;#39;s say that we</text><text start="916.9" dur="1.09">again want to find the</text><text start="918.05" dur="1.37">total value of the</text><text start="919.7" dur="1.54">accesses for each domain, but now</text><text start="921.53" dur="1.46">the value is computed using</text><text start="923.53" dur="3.14">the user attributes that we get from the separate data set.</text><text start="927.32" dur="2.43">Well, this frankly, in map reduce, is hard to do.</text><text start="930.35" dur="2.21">It effectively involves joining these</text><text start="932.85" dur="1.45">two data sets, not something</text><text start="934.55" dur="1.66">that&amp;#39;s supported natively in MapReduce.</text><text start="936.57" dur="0.8">So now we&amp;#39;ve kind of hit</text><text start="937.48" dur="1.22">the limit of what&amp;#39;s very</text><text start="938.84" dur="1.05">convenient to do in the</text><text start="940.08" dur="1.55">map reduce framework, but we</text><text start="941.84" dur="2.42">will momentarily see that there are solutions to that as well.</text><text start="945.43" dur="0.97">So, to summarize, the MapReduce</text><text start="946.67" dur="2.19">framework has no built-in data model.</text><text start="949.2" dur="2.07">The data just starts and files and it ends in files.</text><text start="951.74" dur="1.25">The user just needs to</text><text start="953.06" dur="1.5">provide specific functions, the</text><text start="954.66" dur="1.66">map function, reduce function, reader</text><text start="956.6" dur="1.61">and writer and optionally, a combiner.</text><text start="959.08" dur="1.26">And the system will provide all</text><text start="960.58" dur="1.55">of the execution glue, it will</text><text start="962.31" dur="1.69">guarantee the tolerance to</text><text start="964.19" dur="1.41">system failures and it</text><text start="965.8" dur="1.66">provides scalability by doing the</text><text start="968.01" dur="1.05">assignment of the processing tasks,</text><text start="969.59" dur="2.52">to say an increasing number of computing nodes.</text><text start="973.32" dur="1.15">So when the MapReduce framework</text><text start="974.99" dur="1.81">came out of Google and the</text><text start="976.96" dur="3.36">Hadoop open source implementation was released, there&amp;#39;s a lot of excitement.</text><text start="981.34" dur="1.26">It was pretty exciting because you</text><text start="982.7" dur="1.08">could just write a couple of</text><text start="983.91" dur="1.45">simple functions and then</text><text start="985.83" dur="1.62">the system would provide the</text><text start="987.93" dur="1.3">processing of massive amounts of data</text><text start="989.5" dur="1.09">through those functions, and it</text><text start="990.69" dur="1.04">would be scalable, it would be</text><text start="991.79" dur="1.86">efficient, and it would be fault tolerant.</text><text start="994.49" dur="1.59">But over time, people realized</text><text start="996.5" dur="1.21">that they don&amp;#39;t always want that</text><text start="998.26" dur="2.01">low level programming, and our</text><text start="1000.69" dur="1.82">favorite traditional notions of</text><text start="1002.69" dur="1.43">database schemas and declarative</text><text start="1004.55" dur="1.27">queries started to be missed.</text><text start="1006.9" dur="1.19">And so what was developed is</text><text start="1008.57" dur="1.64">some languages that actually sit</text><text start="1010.41" dur="2.13">on top of Hadoop or the MapReduce framework.</text><text start="1013.42" dur="1.04">One of them is called Hive,</text><text start="1014.9" dur="2.18">and Hive offers schemas and</text><text start="1017.23" dur="2.18">a language that looks very much like SQL.</text><text start="1020.27" dur="1.37">Another language is called Pig.</text><text start="1022.02" dur="1.25">Pig is a little bit more imperative.</text><text start="1023.75" dur="0.91">In other words, it&amp;#39;s a bit</text><text start="1024.74" dur="1.37">more of a statement language, but</text><text start="1026.27" dur="1.54">the fundamental constructs in Pig</text><text start="1028.16" dur="2">are still relational operators, and</text><text start="1030.25" dur="1.38">you could almost think of a</text><text start="1031.97" dur="0.86">Pig script as being a little</text><text start="1032.95" dur="1.31">bit like those statements of relational</text><text start="1034.85" dur="1.53">algebra that we saw way</text><text start="1036.72" dur="2.41">back when, with the addition of loops and so forth.</text><text start="1039.9" dur="1.54">Both of these languages are</text><text start="1041.52" dur="1.68">what the user sees, and they</text><text start="1043.34" dur="1.43">compile to a workflow</text><text start="1045.27" dur="1.14">or you think of that as a</text><text start="1046.51" dur="2.34">graph - of Hadoop jobs.</text><text start="1049.59" dur="1.29">Hadoop, again, being the open source</text><text start="1051.14" dur="1.23">implementation of map and</text><text start="1052.5" dur="1.15">reduce, any job being one</text><text start="1054.75" dur="1.28">instance of map and</text><text start="1056.14" dur="2.25">reduce like that big picture I showed before.</text><text start="1059.43" dur="0.93">And one thing I should mention,</text><text start="1060.74" dur="1.53">as of November, 2011, which</text><text start="1062.81" dur="2.04">it is now, a really significant</text><text start="1065.72" dur="2.74">portion of Hadoop jobs</text><text start="1068.56" dur="2.54">are actually generated by Hive and Pig, or Hive or Pig.</text><text start="1071.62" dur="1.13">So more and more users</text><text start="1073.42" dur="1.22">are actually choosing to use</text><text start="1074.93" dur="1.18">a higher level language rather</text><text start="1076.44" dur="1.87">than program the MapReduce framework directly.</text><text start="1079.76" dur="2.62">Now I&amp;#39;d be remiss if I didn&amp;#39;t also mention one other system.</text><text start="1082.78" dur="1.65">There&amp;#39;s a system called Driad that</text><text start="1084.66" dur="2.05">allows users to specify a</text><text start="1086.91" dur="2.14">workflow, sort of similar to</text><text start="1089.11" dur="1.09">the workflow that might be generated by</text><text start="1090.48" dur="1.09">Hive and Pig, so it&amp;#39;s</text><text start="1091.8" dur="2.45">more general than just one MapReduce job.</text><text start="1094.84" dur="1.04">And there&amp;#39;s also a language called</text><text start="1096.21" dur="1.39">Driadlink that sits on</text><text start="1097.78" dur="1.46">top of Driad and compiles</text><text start="1099.91" dur="0.97">to Driad, sort of in the</text><text start="1101.02" dur="1.24">same way that Hive and</text><text start="1102.42" dur="3.01">Pig compile to a workflow of MapReduce jobs.</text><text start="1106.25" dur="2.41">Now let&amp;#39;s move on to talk about key value stores.</text><text start="1109.8" dur="2.28">As a reminder, the Hadoop or</text><text start="1112.12" dur="1.67">MapReduce framework is designed</text><text start="1114.35" dur="2.65">for more OLAP-type operations, or</text><text start="1117.54" dur="2.15">analytical operations that involve scanning</text><text start="1120.04" dur="0.93">most of the data, and I</text><text start="1121.09" dur="2.36">think that was very clear from what the MapReduce framework does.</text><text start="1124.05" dur="1.24">Where key value stores are</text><text start="1125.65" dur="1.37">designed more for these OLTP</text><text start="1128.06" dur="1.27">style applications, where you&amp;#39;re doing</text><text start="1129.77" dur="2.21">small operations, maybe over</text><text start="1132.54" dur="1.76">a single record, in a massive database.</text><text start="1135.67" dur="1.98">And so the key value stores are extremely simple.</text><text start="1138.56" dur="1.73">The data model for key</text><text start="1140.5" dur="1.12">value stores are just pairs</text><text start="1142.28" dur="1.47">of keys and values, not surprisingly.</text><text start="1144.93" dur="2.18">And the basic operations are simply</text><text start="1147.72" dur="1.33">to insert a new record,</text><text start="1149.53" dur="0.87">so you provide a key and</text><text start="1150.53" dur="1.34">value, to fetch a</text><text start="1152.09" dur="1.8">record by it&amp;#39;s key, to update</text><text start="1154.39" dur="1.68">the contents, the value in</text><text start="1156.4" dur="0.86">the record for a given</text><text start="1157.55" dur="2.04">key, or to delete the record with the given key.</text><text start="1160.53" dur="1.23">So, that&amp;#39;s it and with</text><text start="1161.95" dur="1.03">that simple set of operations</text><text start="1163.86" dur="1.75">as you can imagine, the implementation</text><text start="1166.01" dur="1.72">is focusing on doing these</text><text start="1167.96" dur="1.68">simple operations over massive databases</text><text start="1170.43" dur="1.89">very, very quickly. So, again</text><text start="1172.44" dur="1.65">like Hadoop, efficiency, scalability,</text><text start="1174.79" dur="0.98">and fault tolerance are the</text><text start="1175.86" dur="1.37">most important things because we&amp;#39;re</text><text start="1177.47" dur="1.51">looking at applications with massive</text><text start="1179.35" dur="1.36">amounts of data and very stringent</text><text start="1181.39" dur="1.58">performance requirements. So the</text><text start="1183.05" dur="1.37">way the implementation works at</text><text start="1184.49" dur="1.2">a very, very high level, it&amp;#39;s</text><text start="1185.8" dur="1.37">actually quite complicated to make</text><text start="1187.38" dur="1.94">it work very well, is that</text><text start="1189.38" dur="1.21">the records are distributed to the</text><text start="1190.62" dur="1.4">nodes, the computing nodes</text><text start="1192.22" dur="2.48">based on the key, probably a hash value over the key.</text><text start="1195.3" dur="2.61">So to find the record for a given key can be very quick.</text><text start="1198.17" dur="0.74">You go straight to the node.</text><text start="1199.82" dur="1">In fact, the records may be</text><text start="1201.03" dur="1.68">replicated across multiple nodes</text><text start="1203.46" dur="1.22">and that gives you both efficiency,</text><text start="1205.35" dur="1.05">you can go to maybe a lightly</text><text start="1206.6" dur="1.2">loaded node, it gives you</text><text start="1207.89" dur="1.89">fault tolerance if a node fails.</text><text start="1210.94" dur="1.73">The notion of the</text><text start="1212.95" dur="1.68">actions and key value stores are very simple.</text><text start="1214.91" dur="1.62">One operation itself is a</text><text start="1216.58" dur="1.01">transaction, so we don&amp;#39;t have</text><text start="1217.69" dur="2.2">the idea of grouping a bunch of operations into transactions.</text><text start="1223.56" dur="0.68">And furthermore, they implement something called eventual consistency.</text><text start="1224.99" dur="1">And that says that the replicas</text><text start="1227.09" dur="1.2">of a single record can</text><text start="1228.73" dur="1.97">actually diverge in their value for some point of time.</text><text start="1231.5" dur="2.59">What eventual consistency specifies is</text><text start="1234.17" dur="1.79">that if all operations stop,</text><text start="1236.7" dur="1.11">then the system will become</text><text start="1238.15" dur="1.47">consistent with all copies of</text><text start="1239.86" dur="1.31">each record being the same.</text><text start="1242.2" dur="1.58">Now, unfortunately, as is sometimes</text><text start="1244.18" dur="1.63">the case, these very simple operations</text><text start="1246.49" dur="1.6">and this simple data model weren&amp;#39;t</text><text start="1248.42" dur="1.39">always quite enough, and so</text><text start="1250.28" dur="1.12">some key value stores, but not</text><text start="1251.92" dur="1.57">all I would say, have a</text><text start="1254.01" dur="2.22">concept called columns that occur within the value.</text><text start="1256.67" dur="0.94">So the value here has a</text><text start="1257.87" dur="1.33">little bit more structure to</text><text start="1259.39" dur="2.09">it than just a blob of bits.</text><text start="1262.18" dur="1.24">And the columns will typically</text><text start="1263.86" dur="2.64">be kind of like an embedded key value stores.</text><text start="1267.08" dur="2.13">One thing that&amp;#39;s important is they don&amp;#39;t require uniform column.</text><text start="1269.53" dur="0.81">So none of the key value</text><text start="1270.62" dur="1.48">stores are as strict in</text><text start="1272.24" dur="1.25">their structure as a relational</text><text start="1274.02" dur="2.38">database system would be.</text><text start="1276.62" dur="1.2">The other addition that some</text><text start="1278.22" dur="2.12">allow is a fetch on a range of keys.</text><text start="1280.61" dur="1.08">So this might say I want</text><text start="1281.91" dur="1.06">to get all keys say between</text><text start="1283.61" dur="2.32">two and ten, and</text><text start="1286.24" dur="1.26">so that requires a different</text><text start="1287.9" dur="1.13">type of implementation as you can</text><text start="1289.15" dur="1.26">imagine, but it does allow</text><text start="1290.98" dur="0.91">that operation to be performed</text><text start="1292.31" dur="2.41">efficiently if that is something that the application needs.</text><text start="1296.02" dur="1.85">Just a few examples of key value stores.</text><text start="1298.27" dur="1.54">This is not an exhaustive list, there</text><text start="1299.99" dur="0.95">are many more and this is</text><text start="1301.02" dur="1.85">only November 2011, so things</text><text start="1303.13" dur="1.24">will change over time.</text><text start="1305.01" dur="1.82">But some of the</text><text start="1306.89" dur="0.93">more prominent key value stores are listed</text><text start="1308.13" dur="1.8">here - Google&amp;#39;s Big Table, Amazon,</text><text start="1310.46" dur="1.38">Dynamo, Cassandra, which is</text><text start="1311.89" dur="2.06">an open source, Voldemort, H-base,</text><text start="1314.46" dur="2.03">and again there are many others. These are just a few example.</text><text start="1317.85" dur="1.4">Now let&amp;#39;s talk about document stores.</text><text start="1319.69" dur="1.46">Actually document stores are</text><text start="1321.26" dur="1.04">very much like key value</text><text start="1322.64" dur="1.92">stores, except the value itself is a document.</text><text start="1325.59" dur="2.06">So the data model is a</text><text start="1327.72" dur="1.52">key document pairs and</text><text start="1329.84" dur="0.94">what&amp;#39;s interesting now is that</text><text start="1330.92" dur="1.86">the document in document stores</text><text start="1333" dur="1.49">is typically a known type</text><text start="1335.05" dur="1.2">of structure so the document</text><text start="1336.77" dur="2.22">might contain JSON formatted data</text><text start="1339.55" dur="1.53">javascript object notation.</text><text start="1342.01" dur="1.44">It might contain XML, which we</text><text start="1343.57" dur="2.01">have learned about, or other semi-structured formats.</text><text start="1346.62" dur="1.76">The basic operations are very</text><text start="1348.76" dur="1.45">similar though to what we</text><text start="1351.35" dur="0.87">say in key value stores.</text><text start="1352.91" dur="2.15">You can insert a new document based on a key.</text><text start="1355.66" dur="1.26">We can fetch based on a key.</text><text start="1357.6" dur="2.45">Modify the contents associated with key and delete</text><text start="1361.66" dur="2.09">the record associated with a specific key.</text><text start="1364.32" dur="1.71">But also very important is</text><text start="1366.34" dur="0.98">that there is a fetch</text><text start="1367.42" dur="1.12">operation based on the document</text><text start="1369.11" dur="1.38">contents, and this is</text><text start="1370.61" dur="3.1">very system/format specific, what</text><text start="1374.14" dur="1.94">the operations would be. So</text><text start="1376.17" dur="1.89">there is not a standardized fetched</text><text start="1378.47" dur="1.55">query language at this point in time.</text><text start="1381.11" dur="1.38">Again a few example systems,</text><text start="1382.58" dur="1.92">a not exhaustive list,</text><text start="1384.81" dur="1.55">are the systems Couch-DB,</text><text start="1386.9" dur="1.49">Mongo DB, Simple DB.</text><text start="1388.72" dur="2.2">They all seem to have DB in their name.</text><text start="1391.29" dur="0.74">And again, this is November</text><text start="1392.52" dur="1.12">2011, things that will</text><text start="1393.96" dur="1.6">undoubtedly change.</text><text start="1396.4" dur="2.8">One SQL system I&amp;#39;d like to cover is graph database systems.</text><text start="1400.23" dur="1.34">Graph database system, as the</text><text start="1401.68" dur="1.44">name implies, are designed for</text><text start="1403.62" dur="1.64">storing and running queries</text><text start="1405.67" dur="1.52">or other operations over very large</text><text start="1407.42" dur="1.23">graphs, the data model</text><text start="1409.06" dur="1.71">is that every object is</text><text start="1410.91" dur="2.41">either a node or it&amp;#39;s an edge between nodes.</text><text start="1414.36" dur="1.8">Nodes may have properties, very</text><text start="1416.45" dur="1.21">often ID is a</text><text start="1417.7" dur="1.08">required property of a,</text><text start="1418.85" dur="1.69">and edges may have labels</text><text start="1421.2" dur="1.06">so you can think of them as rolls.</text><text start="1423" dur="1.21">So I think what&amp;#39;s best to understand,</text><text start="1424.46" dur="0.8">this is just to see an</text><text start="1425.35" dur="1.26">example. My example is</text><text start="1426.74" dur="1.06">going to be a very small</text><text start="1428.22" dur="1.21">social network, a tiny one actually.</text><text start="1429.98" dur="1.37">A similar one to what</text><text start="1431.77" dur="1.48">was used for some of our SQL exercises.</text><text start="1434.35" dur="1.31">So let&amp;#39;s start with three</text><text start="1436.06" dur="1.11">nodes, and the nodes are</text><text start="1437.24" dur="1.46">gonna represent people, and the</text><text start="1438.82" dur="1.3">properties of the nodes are</text><text start="1440.25" dur="2.65">going to be ID, name, and grade.</text><text start="1443.62" dur="1.26">And so each node is</text><text start="1445.04" dur="2.81">going to have a value for the ID, name, and grade.</text><text start="1448.33" dur="1.33">For this one, we&amp;#39;ll make</text><text start="1450.08" dur="1.79">it one, Amy in</text><text start="1452.04" dur="2.16">grade nine, and we&amp;#39;ll have two more.</text><text start="1455.23" dur="1.37">So here are the three nodes representing</text><text start="1457.34" dur="1.5">three people in our social graph.</text><text start="1459.25" dur="1.88">We also have ID2, which</text><text start="1461.37" dur="2.68">is Ben in grade nine, and ID3, which is Carol in grade ten.</text><text start="1465.23" dur="0.74">Depending on the system, the</text><text start="1465.99" dur="0.97">nodes may or may not have</text><text start="1467.19" dur="1.39">to have uniform key value</text><text start="1468.89" dur="2.46">pairs within the most system won&amp;#39;t be that stringent.</text><text start="1472.41" dur="2.98">Then in addition to the nodes, we have the edges between the nodes.</text><text start="1476.08" dur="1.65">Typically they would be directed edges.</text><text start="1478.64" dur="2.37">So, let&amp;#39;s make two different types of edges.</text><text start="1481.34" dur="2.78">Let&amp;#39;s make friend edges and let&amp;#39;s make likes edges.</text><text start="1484.93" dur="2.83">So, let&amp;#39;s say, for example, Amy likes Ben.</text><text start="1488.19" dur="1.19">So that would be a directed edge</text><text start="1490.03" dur="1.2">here with the property likes,</text><text start="1491.76" dur="1.5">and maybe Ben likes</text><text start="1493.74" dur="1.03">Carol, let&amp;#39;s say here.</text><text start="1494.88" dur="1.95">And maybe then we</text><text start="1496.99" dur="2.19">have that Amy and</text><text start="1499.91" dur="1.68">Carol are both friends with</text><text start="1501.79" dur="1.06">each other so we&amp;#39;ll have</text><text start="1503.06" dur="1.6">a different type of edge called friend.</text><text start="1505.4" dur="1.3">Now, one might wonder how</text><text start="1506.89" dur="1.2">long those friendships will last</text><text start="1508.46" dur="1.15">with this complicated likes relationship.</text><text start="1510.46" dur="0.85">But, in any case, this gives</text><text start="1511.43" dur="1.35">you an idea of the</text><text start="1512.86" dur="1.82">type of data that&amp;#39;s stored in a graph database.</text><text start="1515.21" dur="1.27">The data model is very specifically</text><text start="1517.55" dur="1.7">about storing nodes with properties</text><text start="1519.87" dur="1.3">inside them, like key value</text><text start="1521.45" dur="1.15">pairs and edges</text><text start="1522.85" dur="1.2">typically with labels or rolls</text><text start="1524.79" dur="0.99">on them, of course that&amp;#39;s not required.</text><text start="1527.24" dur="1.72">So in graph database systems, currently,</text><text start="1529.75" dur="0.96">the interfaces to the systems</text><text start="1531.16" dur="1.55">and the query languages vary a lot.</text><text start="1532.97" dur="1.38">There&amp;#39;s no standardization at all</text><text start="1535.38" dur="1.4">and the queries might just be</text><text start="1537.17" dur="1.77">single-step queries like asking for friends.</text><text start="1539.66" dur="1.9">They might be path expressions like,</text><text start="1541.91" dur="2.47">ask for the women friends, of the men friends of someone.</text><text start="1544.7" dur="1.4">We saw that example in the earlier video.</text><text start="1546.84" dur="1.23">Or they might have full recursion, where</text><text start="1548.26" dur="1.66">you can traverse to arbitrary</text><text start="1550.69" dur="0.79">depths through the graph.</text><text start="1552.9" dur="2.43">A few example systems, again, as</text><text start="1555.44" dur="0.77">of November, 2011, you know I</text><text start="1556.42" dur="0.82">was going to say that, are</text><text start="1557.3" dur="3.36">a Neo4J, Flat DB and Prego.</text><text start="1561.19" dur="2.03">And these systems actually differ quite a lot from each other.</text><text start="1563.92" dur="1.26">I also wanted to mention RDF.</text><text start="1566.01" dur="1.61">RDF is the resource description</text><text start="1568.3" dur="1.49">framework, and there&amp;#39;s something</text><text start="1570" dur="2.27">known as the RDF triple stores.</text><text start="1573.01" dur="2.12">RDF is based on objects</text><text start="1575.48" dur="1.57">having relationships to other objects.</text><text start="1577.35" dur="0.84">So you can almost think of those</text><text start="1578.41" dur="1.33">as two nodes with edges</text><text start="1580.06" dur="0.88">between them, so you can</text><text start="1581.2" dur="2.76">imagine how RDF can be mapped to graph databases.</text><text start="1585.45" dur="2.45">So, those were four examples of NoSQL systems.</text><text start="1589.01" dur="1.01">If the most prominent categories</text><text start="1590.6" dur="1.07">at this point in time, the</text><text start="1591.75" dur="2.12">MapReduce framework, again with</text><text start="1594.41" dur="1.18">languages sitting on top</text><text start="1595.79" dur="1.01">of MapReduce such as</text><text start="1596.98" dur="1.64">Hive and Pig, key value</text><text start="1599.09" dur="1.62">stores for more small</text><text start="1600.98" dur="2.13">transactions over massive databases</text><text start="1603.65" dur="1.58">but just operating small bits of them at once.</text><text start="1606" dur="2.34">Document stores, and graph database systems.</text><text start="1609.18" dur="1.23">NoSQL stands for not</text><text start="1610.91" dur="1.86">only sql, recognizing that</text><text start="1612.91" dur="1.83">for some applications these frameworks</text><text start="1615.28" dur="1.75">work better than traditional database</text><text start="1617.49" dur="1.11">systems, but for many applications</text><text start="1619.4" dur="1.52">- a vast number of</text><text start="1620.99" dur="2.25">applications - traditional databases are still used.</text></transcript>